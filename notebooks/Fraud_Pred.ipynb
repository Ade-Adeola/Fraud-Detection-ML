{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UumgHhTBepvf"
   },
   "source": [
    "Importing some necessary libraries that will be useful for my data analysis, visualization and machine learning. Other necessary libraries will be installed when I come accross the need for them during the process. 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrIlq8vJGFNH"
   },
   "source": [
    "### Setup\n",
    "I import the libraries I need (pandas/NumPy for data, matplotlib/seaborn for plots, scikit‑learn for ML) and silence non‑critical warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0_Jcp8IKQT8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEFbrQIiXTSC"
   },
   "source": [
    "## IMPORTING & INSPECTING DATASET\n",
    "\n",
    "#### At this stage, I am only performing a light inspection of the dataset to understand its shape, missing values, and distributions. I will postpone deeper analysis (skewness, scaling needs, final feature selection) until after I clean the data and impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgmoYin-GFNJ"
   },
   "source": [
    "### Load data\n",
    "I load the training/test CSVs and preview shapes/heads to confirm they read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhslH-GBMcVH"
   },
   "outputs": [],
   "source": [
    "dt = pd.read_csv('/content/drive/MyDrive/fraud_transactions_train_10000_with_missing.csv')\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljKOT7j2MzWf"
   },
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_1L2gZBX9nW"
   },
   "source": [
    "## DROPPING ID COLUMNS\n",
    "\n",
    "#### ID columns are not part of the features are not useful for the predictive analysis so i'll be dropping them. 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn_IAqDZYjRJ"
   },
   "outputs": [],
   "source": [
    "dt.drop(['transaction_id', 'customer_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCfBzAugd2tA"
   },
   "source": [
    "#### Defining a function that helps me check for the percentage of missingness across the entire dataset.👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A-HbPd0GFNL"
   },
   "source": [
    "### Missing values quick check\n",
    "I compute % missing per column so I can plan imputation (remember: keep missingness as signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRBmWreqdyrj"
   },
   "outputs": [],
   "source": [
    "def perc_missing(df):                                  # defining a function for checking % missing values of any dataset\n",
    "  missing = round((df.isnull().sum()/len(df))*100,3)   # this code is replicating the formular (sum of null values/total values) * 100, and rounding up to 3 decimal places\n",
    "  perc_missing = missing[missing>0].sort_values()      # this code is to select from the data only the columns with missing values more than 0\n",
    "\n",
    "  return perc_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCTV6zefYkR"
   },
   "outputs": [],
   "source": [
    "perc_missing(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lsl999-Cg7Pa"
   },
   "source": [
    "#### From the outcome, it can be observed that three columns have missing values with percentage missingness if 2%, 3% and 5% respectively.👆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks9jlU1hnYTx"
   },
   "source": [
    "#### Inspecting the count of unique values across all columns for deciding the best encoding methods later on. 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xewMDzRYl2UR"
   },
   "outputs": [],
   "source": [
    "for col in dt.select_dtypes(include='object').columns:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(dt[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXx63o0_oWvS"
   },
   "source": [
    "## SPLITTING THE DATASET AS EARLY AS POSSIBLE\n",
    "\n",
    "#### Splitting to X and Y, Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6CEF7w0pl4k"
   },
   "outputs": [],
   "source": [
    "X = dt.iloc[:,:-1]\n",
    "y =dt.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBqL3SFgo6i-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPJcHp-PqXrt"
   },
   "source": [
    "#### I'll split to 80/20 so that I will have more data to train on since the fraud cases are usually rare. 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsVpXxk2GFNM"
   },
   "source": [
    "### Early split to avoid leakage\n",
    "I split into train/test now so any fitting (imputation/encoding/scaling) only learns from train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrGdMfWko-1q"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Bj8OvloQgyJ"
   },
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCkKSTZXQsSr"
   },
   "outputs": [],
   "source": [
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTDn-U7NQ4PZ"
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWls3vvyQ-ZC"
   },
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W_IHG70W9Rk"
   },
   "source": [
    "## CLEANING DATASET\n",
    "\n",
    "## Handling Missing Values\n",
    "\n",
    "#### In this fraud prediction project, I decided not to drop any rows or columns that contain missing values. The reason is that every transaction record is potentially important for identifying fraudulent activity, and removing rows may eliminate rare but critical fraud cases.\n",
    "\n",
    "#### Similarly, dropping columns is not advisable because even features with missing values can carry useful signals. For example, the fact that a customer did not provide income information, or that device trust data is unavailable, could itself correlate with fraudulent behavior.\n",
    "\n",
    "#### Instead of dropping, I will handle missing values through imputation strategies (such as median filling for numerical features and special categories/flags for categorical ones). This ensures that:\n",
    "\n",
    "\t•\tNo valuable transaction records are lost.\n",
    "\t•\tMissingness itself can be captured and used by the model as a potential fraud indicator.\n",
    "\n",
    "\n",
    "#### In this project, I decided not to apply feature selection before training. The dataset contains 27 features, and in fraud detection every feature can potentially hold weak but important signals of fraudulent behavior. Dropping features too early may lead to losing valuable information, especially since fraud cases are rare and subtle.\n",
    "\n",
    "#### Instead, I will train the models using all 27 features. After training, I will rely on model-based interpretability methods such as feature importance (from tree-based models), coefficients (from logistic regression), and SHAP values to analyze which features contributed most to fraud detection.\n",
    "\n",
    "#### This approach ensures that I do not prematurely discard useful signals. It also allows me to provide insights later about which features were most influential in predicting fraud, without limiting the learning ability of the model at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ryKTSfLWHn2"
   },
   "outputs": [],
   "source": [
    "# First, I'll group the columns into categorical and numerical columns\n",
    "\n",
    "# Categorical columns are all object type columns\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Numerical columns are all int and float type columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number, 'int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdAHLHRnvhpL"
   },
   "source": [
    "#### The 3 missing columns in the dataset are Customer Income Monthly, Average Transaction Amount (30 days) and Device Trust Score.\n",
    "\n",
    "## IN MY OPINION\n",
    "\n",
    "#### I think filling missing numerical values for a fraud detection dataset with median or mean will disrupt the integrity of the dataset because misingness can also be a factor or a signal for fraudulent activities.\n",
    "\n",
    "#### I will have to examine the range of values in each columns to know which values i will input to fill the missing rows in order to generate an outlier for the machine to understand during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvA5ISIUY8xA"
   },
   "outputs": [],
   "source": [
    "cols_with_missing = [\"customer_income_monthly\",\n",
    "                     \"avg_transaction_amount_30d\",\n",
    "                     \"device_trust_score\"]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Minimum value:\", X_train[col].min())\n",
    "    print(\"Maximum value:\", X_train[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YnR4_VbyZi-"
   },
   "outputs": [],
   "source": [
    "for col in cols_with_missing:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Minimum value:\", X_test[col].min())\n",
    "    print(\"Maximum value:\", X_test[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aTddYtKzTgE"
   },
   "source": [
    "#### **From the outcome, I can see assume the range for each column to be;**\n",
    "\n",
    "#### Customer Income Monthly (0 to 20000) - best outlier value (99999)\n",
    "\n",
    "#### Average Transaction Amount (0 to 5000) - best outlier value (99999)\n",
    "\n",
    "#### Device Trust Score (0 to 1) - best outlier value (-1)**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCSpFEsl224s"
   },
   "outputs": [],
   "source": [
    "# Importing imputation libary\n",
    "\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6s9l7xGGFNP"
   },
   "source": [
    "### Impute with sentinel values\n",
    "For selected numeric columns, I fill missing with out‑of‑range sentinels (e.g., 99999) so the model can learn the pattern of missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPgn77-oyhFr"
   },
   "outputs": [],
   "source": [
    "# Filling with outliers to represent missing values\n",
    "\n",
    "# For Customer Income Monthly (99999)\n",
    "\n",
    "imp_income = SimpleImputer(strategy=\"constant\", fill_value=99999)\n",
    "\n",
    "X_train[[\"customer_income_monthly\"]] = imp_income.fit_transform(X_train[[\"customer_income_monthly\"]])\n",
    "X_test[[\"customer_income_monthly\"]] = imp_income.transform(X_test[[\"customer_income_monthly\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PMZFV5TGFNP"
   },
   "source": [
    "### Impute with sentinel values\n",
    "For selected numeric columns, I fill missing with out‑of‑range sentinels (e.g., 99999) so the model can learn the pattern of missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AsOvWUc3qP0"
   },
   "outputs": [],
   "source": [
    "# For Average Transaction Amount 30 days (99999)\n",
    "\n",
    "imp_avg = SimpleImputer(strategy=\"constant\", fill_value=99999)\n",
    "\n",
    "X_train[[\"avg_transaction_amount_30d\"]] = imp_avg.fit_transform(X_train[[\"avg_transaction_amount_30d\"]])\n",
    "X_test[[\"avg_transaction_amount_30d\"]] = imp_avg.transform(X_test[[\"avg_transaction_amount_30d\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj2q6ofR3wOJ"
   },
   "outputs": [],
   "source": [
    "# For Device Trust Score (-1)\n",
    "\n",
    "imp_trust = SimpleImputer(strategy=\"constant\", fill_value=-1)\n",
    "\n",
    "X_train[[\"device_trust_score\"]] = imp_trust.fit_transform(X_train[[\"device_trust_score\"]])\n",
    "X_test[[\"device_trust_score\"]] = imp_trust.transform(X_test[[\"device_trust_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH2WvFH04dU1"
   },
   "outputs": [],
   "source": [
    "# Confirming\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daml5NWj6NBI"
   },
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERx4ef6x6_AJ"
   },
   "source": [
    "## ENCODING CATEGORICAL COLUMNS\n",
    "\n",
    "#### Since the machine only understands numbers, converting categorical columns to number identifiers will be the next step.\n",
    "\n",
    "### Encoding Choice\n",
    "\n",
    "#### For all my categorical columns, I will be using OrdinalEncoder. After inspecting the dataset, I observed that none of the categorical features have a natural order or hierarchy (e.g., “first class > business class > economy class”). In such cases, OrdinalEncoder can safely act like label encoding, mapping each category to a unique integer.\n",
    "\n",
    "#### I chose OrdinalEncoder instead of:\n",
    "\t•\tOneHotEncoder → this would increase the dimensionality significantly, since my dataset already has many features. I want to avoid unnecessary feature expansion.\n",
    "\t•\tLabelEncoder → mainly designed for target labels and not ideal for multiple feature columns. It also does not handle unseen categories well.\n",
    "\t•\tOther encoders (e.g., Target Encoding) → while powerful, they bring higher risk of data leakage if not carefully cross-validated.\n",
    "\n",
    "#### OrdinalEncoder is simple, compact, and integrates smoothly into a pipeline, which is important since I intend to deploy the final model on Streamlit. This makes it easier to save, reload, and apply the exact same preprocessing during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXuj_GnX6aUQ"
   },
   "outputs": [],
   "source": [
    "# Importing library for encoding\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufzjiA2o9rr8"
   },
   "source": [
    "I have already defined all the 'Object' datatype columns as cat_cols, so I can go ahead to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YkoBiPS9nJi"
   },
   "outputs": [],
   "source": [
    "# Encoding\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_train[cat_cols] = encoder.fit_transform(X_train[cat_cols])\n",
    "X_test[cat_cols] = encoder.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KsJ325V9ezL"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr6EHFbd-bAd"
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38gyuvL-AKTc"
   },
   "source": [
    "## SCALING\n",
    "\n",
    "#### In this project, I intend to created two versions of the dataset:\n",
    "#### 1. Unscaled Data (raw values):\n",
    "\n",
    "\t•\tUsed for tree-based models like Random Forest and XGBoost.\n",
    "\t•\tThese models do not require scaling because they split features based on thresholds.\n",
    "\n",
    "#### 2. Scaled Data (standardized features):\n",
    "\n",
    "\t•\tStandardized to mean = 0 and standard deviation = 1.\n",
    "\t•\tUsed for linear models (e.g., Logistic Regression, SVM) and Neural Networks, which are sensitive to feature magnitudes.\n",
    "\t•\tStandardization ensures that no single feature dominates the learning process simply due to its scale.\n",
    "\n",
    "#### I will train models on both datasets:\n",
    "\n",
    "\t•\tTree models on both unscaled and scaled data (to confirm they are robust to scaling).\n",
    "\t•\tLinear/NN models on the scaled data (since they require it).\n",
    "\n",
    "#### This approach allows me to compare performance across algorithm families while ensuring each model receives data in the form that best suits its learning mechanism.\n",
    "\n",
    "#### Also, to avoid tampering with the colums with missing values outliers, i will excempt them from the columns to be scaled. 👇\n",
    "\n",
    "#### I will also avoid scaling the encoded columns and scale only the genuine continuous numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EuMpvAd-fdF"
   },
   "outputs": [],
   "source": [
    "# I dentifying outlier columns\n",
    "\n",
    "outlier_cols = [\"customer_income_monthly\", \"avg_transaction_amount_30d\", \"device_trust_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQGp48W-EGM0"
   },
   "source": [
    "I have already defined all the 'Float' and 'Int' datatype columns as num_cols, so I can go ahead to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27nRzN99EBS9"
   },
   "outputs": [],
   "source": [
    "# Identifying the genuine continuous numeric columns of the dataset\n",
    "\n",
    "scale_cols = [c for c in num_cols if c not in outlier_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtoaMI8RFgnU"
   },
   "outputs": [],
   "source": [
    "# Making a copy of the two paths\n",
    "\n",
    "X_train_unscaled = X_train.copy()\n",
    "X_test_unscaled  = X_test.copy()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled  = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGh30IkxGKiY"
   },
   "outputs": [],
   "source": [
    "# Importing library for standard scaling\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMBSRR_pGiHt"
   },
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_scaled[scale_cols] = sc.fit_transform(X_train_scaled[scale_cols])\n",
    "X_test_scaled[scale_cols]  = sc.transform(X_test_scaled[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMHLEQ4pHQQf"
   },
   "outputs": [],
   "source": [
    "# Confirming\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUG8ZJD3HW3-"
   },
   "outputs": [],
   "source": [
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-WWbcbPTHZav",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install lazypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcyiSufWGFNj"
   },
   "source": [
    "### Train the model\n",
    "I fit the chosen model/pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw9PDZuvIDEQ"
   },
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "#X_train_unscaled, X_test_unscaled, y_train, y_test\n",
    "clf_us = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None, random_state=42)\n",
    "models_us, preds_us = clf_us.fit(X_train_unscaled, X_test_unscaled, y_train, y_test)\n",
    "\n",
    "print(\"=== LazyPredict on UN-SCALED data (good for trees) ===\")\n",
    "print(models_us.sort_values(by=[\"ROC AUC\",\"Accuracy\"], ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4yW9xIgqlfH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEL-KVJ8GFNj"
   },
   "source": [
    "### Train the model\n",
    "I fit the chosen model/pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI3aMge9IKcn"
   },
   "outputs": [],
   "source": [
    "#X_train_scaled, X_test_scaled, y_train, y_test\n",
    "clf_us = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None, random_state=42)\n",
    "models_us, preds_us = clf_us.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "print(\"=== LazyPredict on UN-SCALED data (good for trees) ===\")\n",
    "print(models_us.sort_values(by=[\"ROC AUC\",\"Accuracy\"], ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzda03yAGFNk"
   },
   "source": [
    "### Evaluate\n",
    "I report Accuracy, Balanced Accuracy, Precision, Recall, F1, ROC AUC, and PR AUC — focusing on recall/PR AUC for fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdqX1gPHJlmI"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             accuracy_score, balanced_accuracy_score,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8x58EJcGFNk"
   },
   "source": [
    "### Threshold sweep\n",
    "I scan several probability cutoffs and pick one that boosts recall at acceptable precision (I later settled around 0.45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQUIs52VNHYY"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Metrics nicely\n",
    "# ===============================\n",
    "\n",
    "def print_metrics(y_true, proba, preds, header=\"\"):\n",
    "    print(\"\\n\" + \"=\"*len(header))\n",
    "    print(header)\n",
    "    print(\"=\"*len(header))\n",
    "    print(f\"Accuracy:           {accuracy_score(y_true, preds):.4f}\")\n",
    "    print(f\"Balanced Accuracy:  {balanced_accuracy_score(y_true, preds):.4f}\")\n",
    "    print(f\"Precision:          {precision_score(y_true, preds, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:             {recall_score(y_true, preds, zero_division=0):.4f}\")\n",
    "    print(f\"F1:                 {f1_score(y_true, preds, zero_division=0):.4f}\")\n",
    "    print(f\"ROC AUC:            {roc_auc_score(y_true, proba):.4f}\")\n",
    "    print(f\"PR  AUC:            {average_precision_score(y_true, proba):.4f}\")\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_true, preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF4azOCFGFNk"
   },
   "source": [
    "### Threshold sweep\n",
    "I scan several probability cutoffs and pick one that boosts recall at acceptable precision (I later settled around 0.45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8CX1NSroAet"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Threshold sweep (see trade-offs)\n",
    "# ===============================\n",
    "\n",
    "def threshold_sweep(y_true, proba, thresholds=(0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5)):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        preds = (proba >= t).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": t,\n",
    "            \"precision\": precision_score(y_true, preds, zero_division=0),\n",
    "            \"recall\":    recall_score(y_true, preds, zero_division=0),\n",
    "            \"f1\":        f1_score(y_true, preds, zero_division=0),\n",
    "            \"bal_acc\":   balanced_accuracy_score(y_true, preds)\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEW8snB_GFNl"
   },
   "source": [
    "### Handle class imbalance Random Forest\n",
    "I set `class_weight='balanced'` so the model pays more attention to rare fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nv-pmj4NoWDW"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1) RandomForest (unscaled) + class_weight\n",
    "# ===============================\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,              # you can tune later (e.g., 8, 12, 16)\n",
    "    class_weight=\"balanced\",     # <<< imbalance handling\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train_unscaled, y_train)\n",
    "proba_rf = rf.predict_proba(X_test_unscaled)[:, 1]\n",
    "preds_rf = (proba_rf >= 0.5).astype(int)\n",
    "print_metrics(y_test, proba_rf, preds_rf, header=\"RandomForest (UNSCALED) + class_weight='balanced'\")\n",
    "\n",
    "print(\"\\nThreshold sweep (RF):\")\n",
    "display(threshold_sweep(y_test, proba_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zau-GUKmGFNl"
   },
   "source": [
    "### Handle class imbalance Logistic Regression\n",
    "I set `class_weight='balanced'` so the model pays more attention to rare fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA-IxBOgpcjk"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2) Logistic Regression (scaled) + class_weight\n",
    "# ===============================\n",
    "lr = LogisticRegression(\n",
    "    C=0.1907,\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=900,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "preds_lr = (proba_lr >= 0.5).astype(int)\n",
    "print_metrics(y_test, proba_lr, preds_lr, header=\"LogisticRegression (SCALED) + class_weight='balanced'\")\n",
    "\n",
    "print(\"\\nThreshold sweep (LR):\")\n",
    "display(threshold_sweep(y_test, proba_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiaW86izGFNm"
   },
   "source": [
    "### Handle class imbalance XGBoost\n",
    "I set `class_weight='balanced'` so the model pays more attention to rare fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIbLNrW3pkSp"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3) XGBoost (unscaled) + scale_pos_weight  (optional)\n",
    "# ===============================\n",
    "\n",
    "# scale_pos_weight ≈ negatives / positives in TRAIN\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "spw = (neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=spw,     # <<< key imbalance control\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "xgb.fit(X_train_unscaled, y_train)\n",
    "proba_xgb = xgb.predict_proba(X_test_unscaled)[:, 1]\n",
    "preds_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "print_metrics(y_test, proba_xgb, preds_xgb, header=f\"XGBoost (UNSCALED) + scale_pos_weight={spw:.2f}\")\n",
    "\n",
    "print(\"\\nThreshold sweep (XGB):\")\n",
    "display(threshold_sweep(y_test, proba_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPdb9l0tGFNl"
   },
   "source": [
    "### Train the model\n",
    "I fit the chosen model/pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRfULB-BpnrP"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Search space: just C (regularization strength)\n",
    "param_dist = {\n",
    "    \"C\": loguniform(1e-3, 1e2),           # sample C between 0.001 and 100\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Base Logistic Regression\n",
    "lr = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Randomized search\n",
    "rs = RandomizedSearchCV(\n",
    "    lr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                   # number of random draws\n",
    "    scoring=\"average_precision\", # PR-AUC scoring\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "rs.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "print(\"Best CV PR-AUC:\", rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZS4U1va8pNs"
   },
   "source": [
    "### Model Selection and Hyperparameter Tuning for Fraud Detection\n",
    "\n",
    "At the onset of this project, I used **LazyPredict** to run multiple algorithms on the dataset with default hyperparameters. The purpose of this was not to accept those results at face value, but to quickly summarize and compare which models showed initial promise. Interestingly, some models reported very high accuracies (around **0.95**).\n",
    "\n",
    "However, in fraud detection, a high accuracy does **not** necessarily mean a good model. This is because fraudulent transactions form a very small minority (around 4% of the dataset). A model could achieve >95% accuracy by simply predicting **“non-fraud”** for almost everything. That is dangerous, because it means many fraudulent activities would be missed.\n",
    "\n",
    "The real goal in fraud detection is not just to predict the majority class correctly, **but to force the model to pay more attention to the minority fraudulent class.** In other words, it is better for the model to sometimes flag a genuine transaction as fraudulent (false positive) than to wrongly classify an actual fraudulent transaction as genuine (false negative). For this reason, I moved to **class_weight=“balanced”** in Logistic Regression, so that the algorithm could give more weight to fraud cases during training.\n",
    "\n",
    "⸻\n",
    "\n",
    "### Metrics Focus\n",
    "\n",
    "Because of the imbalanced nature of the dataset, I evaluated models not just on plain accuracy but on multiple metrics that give a clearer picture:\n",
    "\n",
    "\t•\tAccuracy: Overall correct predictions. In fraud analysis, this number can be misleading if used alone. Typically, we expect 0.70–0.85 to be a reasonable range (since forcing the model to detect fraud usually reduces accuracy).\n",
    "\t•\tMy result: 0.77 (within the expected range).\n",
    "\n",
    "\t•\tBalanced Accuracy: Accounts for imbalance by averaging recall across classes. A good fraud model should push this above 0.60.\n",
    "\t•\tMy result: 0.63 (slightly above baseline, showing the model is learning fraud patterns).\n",
    "\n",
    "\t•\tPrecision (fraud class): Of all predicted frauds, how many were actually fraud. Precision is usually low in fraud problems, often <0.2, because the model prefers to “over-flag.”\n",
    "\t•\tMy result: 0.097 (low but acceptable in fraud context, since recall is prioritized).\n",
    "\n",
    "\t•\tRecall (fraud class): Of all actual frauds, how many were caught. This is critical in fraud detection — values around 0.40–0.60 are realistic for first models.\n",
    "\t•\tMy result: 0.48 (good, the model catches nearly half of frauds).\n",
    "\n",
    "\t•\tF1 Score: Harmonic mean of precision and recall. Expected to be low when fraud is rare, but still useful as a balance check.\n",
    "\t•\tMy result: 0.16 (low, but consistent with the recall–precision trade-off).\n",
    "\n",
    "\t•\tROC AUC: Measures the ability to rank frauds above non-frauds. A baseline is 0.50 (random). Values between 0.60–0.70 are acceptable in early fraud work.\n",
    "\t•\tMy result: 0.64 (model is better than random and shows a signal).\n",
    "\n",
    "\t•\tPR AUC: More honest for rare classes because it focuses on precision–recall trade-off. Baseline equals fraud rate (~0.04). Anything above 0.07–0.08 shows the model is learning.\n",
    "\t•\tMy result: 0.078 (almost double the baseline, good progress).\n",
    "\n",
    "\t•\tClassification Report: Gave a detailed breakdown for each class, confirming that the model sacrifices precision to improve recall, which is the safer option in fraud detection.\n",
    "\n",
    "⸻\n",
    "\n",
    "### Summary\n",
    "\n",
    "After comparing multiple models, I found that **Logistic Regression with class_weight=“balanced”** was the best-performing and most interpretable model for this task. Hyperparameter tuning (specifically on the C parameter) further improved performance. The final model reached:\n",
    "\n",
    "\t•\tAccuracy = 0.77\n",
    "\t•\tBalanced Accuracy = 0.63\n",
    "\t•\tRecall (fraud class) = 0.48\n",
    "\t•\tROC AUC = 0.64\n",
    "\t•\tPR AUC = 0.078\n",
    "\n",
    "These results are consistent with what is expected in fraud prediction:\n",
    "\n",
    "\t•\tNot extremely high accuracy (because we forced it to detect fraud).\n",
    "\t•\tReasonable recall (almost half of frauds caught).\n",
    "\t•\tPR AUC above the baseline fraud rate, showing the model has learned useful patterns.\n",
    "\n",
    "⸻\n",
    "\n",
    "This reasoning and explanation justify why Logistic Regression was chosen as the final model, and why the metrics prove it is suitable for fraud detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCRXvYKu9Mkw"
   },
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGi8fifCziKq"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline as SkPipe\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLvGR8sRPYml"
   },
   "source": [
    "**Preprocess 👇**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A93D6GdGFNn"
   },
   "source": [
    "### Impute with sentinel values\n",
    "For selected numeric columns, I fill missing with out‑of‑range sentinels (e.g., 99999) so the model can learn the pattern of missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDKqE6NjAPDU"
   },
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_cols),\n",
    "        (\"imp_income\", SimpleImputer(strategy=\"constant\", fill_value=99999), [\"customer_income_monthly\"]),\n",
    "        (\"imp_avg30\",  SimpleImputer(strategy=\"constant\", fill_value=99999), [\"avg_transaction_amount_30d\"]),\n",
    "        (\"imp_trust\",  SimpleImputer(strategy=\"constant\", fill_value=-1),    [\"device_trust_score\"]),\n",
    "        (\"scale_num\",  SkPipe([(\"scaler\", StandardScaler())]),               scale_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htZw4nCmPo76"
   },
   "source": [
    "**Classifier 👇**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxwGp7OcGFNn"
   },
   "source": [
    "### Handle class imbalance\n",
    "I set `class_weight='balanced'` so the model pays more attention to rare fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJGNvNF7Pkqy"
   },
   "outputs": [],
   "source": [
    "BEST_C = 0.1907\n",
    "clf = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    penalty=\"l2\",\n",
    "    class_weight=\"balanced\",\n",
    "    C=BEST_C,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mYj3y-WQNhG"
   },
   "source": [
    "**Preprocess to model; fit & quick evaluation 👇**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVFMGVM8GFNo"
   },
   "source": [
    "### Train the model\n",
    "I fit the chosen model/pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HDSYB8qQMlO"
   },
   "outputs": [],
   "source": [
    "pipe = SkPipe([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "preds = (proba >= 0.50).astype(int)   # default; you can change later\n",
    "\n",
    "print(\"\\n=== Logistic Regression Pipeline (t=0.50) ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, preds), 4))\n",
    "print(\"Balanced Acc:\", round(balanced_accuracy_score(y_test, preds), 4))\n",
    "print(\"Precision:\", round(precision_score(y_test, preds, zero_division=0), 4))\n",
    "print(\"Recall:\", round(recall_score(y_test, preds, zero_division=0), 4))\n",
    "print(\"F1:\", round(f1_score(y_test, preds, zero_division=0), 4))\n",
    "print(\"ROC AUC:\", round(roc_auc_score(y_test, proba), 4))\n",
    "print(\"PR  AUC:\", round(average_precision_score(y_test, proba), 4))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX5vVdi3GFNo"
   },
   "source": [
    "### Threshold sweep\n",
    "I scan several probability cutoffs and pick one that boosts recall at acceptable precision (I later settled around 0.45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuUQS4OYRJjn"
   },
   "outputs": [],
   "source": [
    "for t in [0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]:\n",
    "    p = (proba >= t).astype(int)\n",
    "    print(f\"t={t:.2f}  Prec={precision_score(y_test,p,zero_division=0):.3f}  \"\n",
    "          f\"Rec={recall_score(y_test,p,zero_division=0):.3f}  \"\n",
    "          f\"BalAcc={balanced_accuracy_score(y_test,p):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZUv-grKSU2a"
   },
   "source": [
    "**Saving the model + threshold 👇**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCui4oZHGFNo"
   },
   "source": [
    "### Setup\n",
    "I import the libraries I need (pandas/NumPy for data, matplotlib/seaborn for plots, scikit‑learn for ML) and silence non‑critical warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBo3y6yVSymH"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsONWGyMGFNp"
   },
   "source": [
    "### Saving pipeline in pkl for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiyPwgFBRp7l"
   },
   "outputs": [],
   "source": [
    "CHOSEN_THRESHOLD = 0.45   # I selected the best threshold from the threshold sweep result.\n",
    "\n",
    "# combining pipeline + threshold together\n",
    "\n",
    "artifacts = {\n",
    "    \"pipeline\": pipe,\n",
    "    \"threshold\": CHOSEN_THRESHOLD\n",
    "}\n",
    "\n",
    "with open(\"fraud_threshold.pkl\", \"wb\") as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "print(\"Saved fraud_lr_pipeline.pkl (pipeline + threshold together)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "majszp5XGFNL"
   },
   "source": [
    "## QUICK DRIFT TEST CHECK ON SOME IMPORTANT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIYCK5QBS7bA"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "features_to_check = [\"transaction_amount\", \"customer_income_monthly\", \"device_trust_score\"]\n",
    "\n",
    "for col in features_to_check:\n",
    "    stat, p = ks_2samp(X_train[col].dropna(), X_test[col].dropna())\n",
    "    print(f\"{col} → KS test p-value: {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"  ⚠️ Possible drift detected\")\n",
    "    else:\n",
    "        print(\"  ✅ No significant drift\")\n",
    "\n",
    "    # plot histogram\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(X_train[col], bins=30, alpha=0.5, label='Train')\n",
    "    plt.hist(X_test[col], bins=30, alpha=0.5, label='Test')\n",
    "    plt.title(f\"Distribution comparison for {col}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
